## k8s安装部署

## 1.k8s安装

```bash
#每个节点都安装
yum -y install kubeadm-1.18.2 kubelet-1.18.2 kubectl-1.18.2
systemctl enable kubelet && systemctl daemon-reload
#命令参数自动补全
 yum install -y bash-completion
 source /usr/share/bash-completion/bash_completion
 source <(kubectl completion bash)
 echo "source <(kubectl completion bash)" >> ~/.bashrc
```

2.配置文件

```bash
# k8s-master-01配置文件
cd /etc/kubernetes/manifests
cat > kubeadm-config.yaml <<EOF
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
imageRepository: registry.aliyuncs.com/google_containers
kubernetesVersion: v1.18.2
networking: 
  dnsDomain: cluster.local  
  podSubnet: 10.244.0.0/16
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: "master.k8s.io:16443"
apiServer:
  certSANs:
    - k8s-master-01
    - k8s-master-02
    - k8s-master-03
    - master.k8s.io
    - 10.41.3.201
    - 10.41.3.202
    - 10.41.3.208
    - 10.41.3.211
    - 127.0.0.1
  extraArgs:
    authorization-mode: Node,RBAC
  extraVolumes:
  - hostPath: /etc/localtime
    mountPath: /etc/localtime
    name: localtime
    readOnly: true  
  timeoutForControlPlane: 4m0s
#controllerManager: {}
controllerManager: # https://godoc.org/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta2#ControlPlaneComponent
  extraArgs:
    bind-address: "0.0.0.0"
    experimental-cluster-signing-duration: 876000h
  extraVolumes:
  - hostPath: /etc/localtime
    mountPath: /etc/localtime
    name: localtime
    readOnly: true
dns: 
  type: CoreDNS
etcd:
  local:    
    dataDir: /var/lib/etcd
#scheduler: {}
scheduler:
  extraArgs:
    bind-address: "0.0.0.0"
  extraVolumes:
  - hostPath: /etc/localtime
    mountPath: /etc/localtime
    name: localtime
    readOnly: true
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: ipvs # or iptables
ipvs:
  excludeCIDRs: null
  minSyncPeriod: 0s
  scheduler: "rr" # 调度算法
  syncPeriod: 15s
iptables:
  masqueradeAll: true
  masqueradeBit: 14
  minSyncPeriod: 0s
  syncPeriod: 30s
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration # https://godoc.org/k8s.io/kubelet/config/v1beta1#KubeletConfiguration
cgroupDriver: systemd
failSwapOn: true # 如果开启swap则设置为false
EOF
```

## 3.下载相关文件

```bash
kubeadm config images pull --config kubeadm-config.yaml
```

## 4.初始化集群

```bash
kubeadm init --config kubeadm-config.yaml --upload-certs(加了过后，后续节点加入自动分发证书)
#初始化完后执行
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

###执行结果
ubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join master.k8s.io:16443 --token a0dpil.9q5pp6ib9yprj8qp \
    --discovery-token-ca-cert-hash sha256:bdb86dcfe6554005ed9fd9f22996148eef2dc064ca7ae9b3fe6d6f70a75f5987 \
    --control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join master.k8s.io:16443 --token a0dpil.9q5pp6ib9yprj8qp \
    --discovery-token-ca-cert-hash sha256:bdb86dcfe6554005ed9fd9f22996148eef2dc064ca7ae9b3fe6d6f70a75f5987 
```

## 5.证书加入到其他master

```bash
#其他两个节点执行
mkdir -p /etc/kubernetes/pki/etcd  
#master-01执行,将证书拷贝到k8s-master-02
ssh-copy-id -i ~/.ssh/id_rsa.pub 10.41.3.202
scp -P 51020 /etc/kubernetes/pki/ca.* root@10.41.3.202:/etc/kubernetes/pki/
scp -P 51020 /etc/kubernetes/pki/sa.* root@10.41.3.202:/etc/kubernetes/pki/
scp -P 51020 /etc/kubernetes/pki/front-proxy-ca.* root@10.41.3.202:/etc/kubernetes/pki/
scp -P 51020 /etc/kubernetes/pki/etcd/ca.* root@10.41.3.202:/etc/kubernetes/pki/etcd/
scp -P 51020 /etc/kubernetes/admin.conf root@10.41.3.202:/etc/kubernetes/
#master-01执行,将证书拷贝到k8s-master-03
ssh-copy-id -i ~/.ssh/id_rsa.pub 10.41.3.208
scp -P 51020 /etc/kubernetes/pki/ca.* root@10.41.3.208:/etc/kubernetes/pki/
scp -P 51020 /etc/kubernetes/pki/sa.* root@10.41.3.208:/etc/kubernetes/pki/
scp -P 51020 /etc/kubernetes/pki/front-proxy-ca.* root@10.41.3.208:/etc/kubernetes/pki/
scp -P 51020 /etc/kubernetes/pki/etcd/ca.* root@10.41.3.208:/etc/kubernetes/pki/etcd/
scp -P 51020 /etc/kubernetes/admin.conf root@10.41.3.208:/etc/kubernetes/

```

## 6.master加入集群

```bash
##分别在master2 master3执行
kubeadm join master.k8s.io:16443 --token a0dpil.9q5pp6ib9yprj8qp \
    --discovery-token-ca-cert-hash sha256:bdb86dcfe6554005ed9fd9f22996148eef2dc064ca7ae9b3fe6d6f70a75f5987 \
    --control-plane 
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config    
 #加入环境变量
 echo "export KUBECONFIG=/etc/kubernetes/admin.conf" >> ~/.bash_profile
 source .bash_profile
```

## 7.node加入集群

```bash
#分别在node1 node2执行
kubeadm join master.k8s.io:16443 --token a0dpil.9q5pp6ib9yprj8qp \
    --discovery-token-ca-cert-hash sha256:bdb86dcfe6554005ed9fd9f22996148eef2dc064ca7ae9b3fe6d6f70a75f5987 
```

## 8.安装网络插件flannel

```bash
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
```

 ## 9.查看节点状态

```bash
[root@master03 pki]# kubectl get nodes
NAME            STATUS   ROLES    AGE   VERSION
k8s-master-01   Ready    master   16h   v1.18.2
k8s-master-02   Ready    master   15h   v1.18.2
k8s-master-03   Ready    master   15h   v1.18.2
k8s-node-01     Ready    <none>   17m   v1.18.2
k8s-node-02     Ready    <none>   11m   v1.18.2

[root@master03 pki]# kubectl get pods -n kube-system
NAME                                    READY   STATUS    RESTARTS   AGE
coredns-7ff77c879f-lrzcs                1/1     Running   0          16h
coredns-7ff77c879f-nd7np                1/1     Running   0          16h
etcd-k8s-master-01                      1/1     Running   0          16h
etcd-k8s-master-02                      1/1     Running   0          15h
etcd-k8s-master-03                      1/1     Running   0          15h
kube-apiserver-k8s-master-01            1/1     Running   0          16h
kube-apiserver-k8s-master-02            1/1     Running   0          15h
kube-apiserver-k8s-master-03            1/1     Running   0          15h
kube-controller-manager-k8s-master-01   1/1     Running   3          16h
kube-controller-manager-k8s-master-02   1/1     Running   0          15h
kube-controller-manager-k8s-master-03   1/1     Running   0          15h
kube-flannel-ds-2dnjr                   1/1     Running   0          16h
kube-flannel-ds-8hhmd                   1/1     Running   0          15h
kube-flannel-ds-mqf5r                   1/1     Running   0          12m
kube-flannel-ds-ppmkn                   1/1     Running   0          18m
kube-flannel-ds-rx8j8                   1/1     Running   0          15h
kube-proxy-47qg9                        1/1     Running   0          16h
kube-proxy-9cz45                        1/1     Running   0          12m
kube-proxy-h7w5m                        1/1     Running   0          15h
kube-proxy-hmts5                        1/1     Running   0          18m
kube-proxy-r7bb5                        1/1     Running   0          15h
kube-scheduler-k8s-master-01            1/1     Running   2          16h
kube-scheduler-k8s-master-02            1/1     Running   0          15h
kube-scheduler-k8s-master-03            1/1     Running   0          15h
```



## 10.部署dashboard

### 1.部署过程

```bash
#1.下载recommended.yaml文件
wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml
#2.修改nodeport
---
kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
spec:
  type: NodePort #增加
  ports:
    - port: 443
      targetPort: 8443
      nodePort: 30000 #增加
  selector:
    k8s-app: kubernetes-dashboard
---
#3.创建证书
mkdir dashboard-certs
cd dashboard-certs/
#创建命名空间
kubectl create namespace kubernetes-dashboard
# 创建key文件
openssl genrsa -out dashboard.key 2048
#证书请求
openssl req -days 36000 -new -out dashboard.csr -key dashboard.key -subj '/CN=dashboard-cert'
#自签证书
openssl x509 -req -in dashboard.csr -signkey dashboard.key -out dashboard.crt
#创建kubernetes-dashboard-certs对象
kubectl create secret generic kubernetes-dashboard-certs --from-file=dashboard.key --from-file=dashboard.crt -n kubernetes-dashboard


#4.安装dashboard
kubectl apply -f recommended.yaml


#5.创建dashboard管理员
vim dashboard-admin.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: dashboard-admin
  namespace: kubernetes-dashboard
 kubectl apply -f dashboard-admin.yaml 
  
#6.分配权限
vim dashboard-admin-bind-cluster-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: dashboard-admin-bind-cluster-role
  labels:
    k8s-app: kubernetes-dashboard
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: dashboard-admin
  namespace: kubernetes-dashboard
kubectl apply -f dashboard-admin-bind-cluster-role.yaml

#7.查看并复制用户Token
[root@k8s-master-01 k8s]# kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep dashboard-admin | awk '{print $1}')
Name:         dashboard-admin-token-tkh5s
Namespace:    kubernetes-dashboard
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: dashboard-admin
              kubernetes.io/service-account.uid: 4db80cef-922e-4974-b0d8-e43e9875e48f

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  20 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IkpnakZDRTVHTWVWTkstWkJQNXBJREYtaVMyM3NvOHNrOXVwRzFfRmNNMkkifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tdGtoNXMiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiNGRiODBjZWYtOTIyZS00OTc0LWIwZDgtZTQzZTk4NzVlNDhmIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.fLud9V64ZvOAbi1n07eeBUo1Zc0QlT_9QDtAfpc174fYo8HkjFxKFF3XHpr_Rxb92qe2dBtFstkBgRWGYJZfYAtQyurR-d20X2unzMOR_gO2IFRB3XPLch4GN0Klo0sMz7imZyKje0Gy7omiYMq2MvOVTYW9CnEUpPvQR0Juxn6c1ONVlH-BhcfY51TvX_Pgb3Ux1OpsXvJMzr_CyUAgeJIBg_IKh_xsUDzACw4gvseSXoGARZHv3hbPsJL9_WMNGws4f0l6ielFQxF6YSKFNZ1WsujVIAXq9ysmLmAGZ8wHf2w3xrwMwMZq8wKfrDzCGir3Kd9BhMRaSzApxVefRQ

#8.通过https://10.41.3.211:30000/ ,用生成的token 登录查看

  
```

### 2.部署结果

![image-20211208114426198](C:\Users\maowan\AppData\Roaming\Typora\typora-user-images\image-20211208114426198.png)





